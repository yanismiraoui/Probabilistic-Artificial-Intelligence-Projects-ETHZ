{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T07:24:41.674564Z",
     "start_time": "2021-10-15T07:24:40.052431Z"
    }
   },
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd \n",
    "\n",
    "import ipywidgets\n",
    "from ipywidgets import interact\n",
    "import IPython\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# If in your browser the figures are not nicely vizualized, change the following line.\n",
    "rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T07:24:42.301853Z",
     "start_time": "2021-10-15T07:24:42.244422Z"
    }
   },
   "outputs": [],
   "source": [
    "def regression_function(x, noise=1e-1):\n",
    "    \"\"\"Get function value.\"\"\"\n",
    "    return torch.sin(2 * x) / x + noise * torch.randn(len(x))\n",
    "\n",
    "\n",
    "class ExactGP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, kernel):\n",
    "        super().__init__(train_x, train_y, likelihood=gpytorch.likelihoods.GaussianLikelihood())\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward computation of GP.\"\"\"\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    @property\n",
    "    def output_scale(self):\n",
    "        \"\"\"Get output scale.\"\"\"\n",
    "        return self.covar_module.outputscale\n",
    "\n",
    "    @output_scale.setter\n",
    "    def output_scale(self, value):\n",
    "        \"\"\"Set output scale.\"\"\"\n",
    "        if not isinstance(value, torch.Tensor):\n",
    "            value = torch.tensor([value])\n",
    "        self.covar_module.outputscale = value\n",
    "        \n",
    "    @property\n",
    "    def length_scale(self):\n",
    "        \"\"\"Get length scale.\"\"\"\n",
    "        ls = self.covar_module.base_kernel.kernels[0].lengthscale\n",
    "        if ls is None:\n",
    "            ls = torch.tensor(0.0)\n",
    "        return ls \n",
    "\n",
    "    @length_scale.setter\n",
    "    def length_scale(self, value):\n",
    "        \"\"\"Set length scale.\"\"\"\n",
    "        if not isinstance(value, torch.Tensor):\n",
    "            value = torch.tensor([value])\n",
    "        \n",
    "        try: \n",
    "            self.covar_module.lengthscale = value \n",
    "        except RuntimeError:\n",
    "            pass \n",
    "        \n",
    "        try:\n",
    "            self.covar_module.base_kernel.lengthscale = value\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "    \n",
    "        try:\n",
    "            for kernel in self.covar_module.base_kernel.kernels:\n",
    "                kernel.lengthscale = value \n",
    "        except RuntimeError:\n",
    "            pass\n",
    "    \n",
    "\n",
    "def get_kernel(kernel, composition=\"addition\"):\n",
    "    base_kernel = []\n",
    "    if \"RBF\" in kernel:\n",
    "        base_kernel.append(gpytorch.kernels.RBFKernel())\n",
    "    if \"linear\" in kernel:\n",
    "        base_kernel.append(gpytorch.kernels.LinearKernel())\n",
    "    if \"quadratic\" in kernel:\n",
    "        base_kernel.append(gpytorch.kernels.PolynomialKernel(power=2))\n",
    "    if \"Matern-1/2\" in kernel:\n",
    "        base_kernel.append(gpytorch.kernels.MaternKernel(nu=1/2))\n",
    "    if \"Matern-3/2\" in kernel:\n",
    "        base_kernel.append(gpytorch.kernels.MaternKernel(nu=3/2))\n",
    "    if \"Matern-5/2\" in kernel:\n",
    "        base_kernel.append(gpytorch.kernels.MaternKernel(nu=5/2))\n",
    "    if \"Cosine\" in kernel:\n",
    "        base_kernel.append(gpytorch.kernels.CosineKernel())\n",
    "\n",
    "    if composition == \"addition\":\n",
    "        base_kernel = gpytorch.kernels.AdditiveKernel(*base_kernel)\n",
    "    elif composition == \"product\":\n",
    "        base_kernel = gpytorch.kernels.ProductKernel(*base_kernel)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    kernel = gpytorch.kernels.ScaleKernel(base_kernel)\n",
    "    return kernel \n",
    "\n",
    "def plot_model(model, train_x, train_y, test_x, inducing_points=None, plot_points=True):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(test_x)\n",
    "        lower, upper = out.confidence_region()\n",
    "        y_dist = model.likelihood(out)\n",
    "        y_lower, y_upper = y_dist.confidence_region()\n",
    "        \n",
    "    if plot_points:\n",
    "        plt.plot(train_x, train_y, 'k*', label='Train Data')\n",
    "    \n",
    "    plt.plot(test_x, out.mean, 'b-', label='Mean Prediction')\n",
    "    plt.plot(test_x, out.sample(), 'r--', label='GP Sample')\n",
    "    for _ in range(3):\n",
    "        plt.plot(test_x, out.sample(), 'r--')\n",
    "    plt.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), \n",
    "                     color='b', alpha=0.2, label='Epistemic Uncertainty')\n",
    "    \n",
    "    \n",
    "    plt.fill_between(test_x.numpy(), y_lower.numpy(), lower.numpy(), \n",
    "                     color='g', alpha=0.2, label='Aleatoric Uncertainty')\n",
    "    plt.fill_between(test_x.numpy(), upper.numpy(), y_upper.numpy(), \n",
    "                     color='g', alpha=0.2)\n",
    "    \n",
    "    plt.ylim([-2, 3.])\n",
    "    if inducing_points is not None:\n",
    "        plt.plot(\n",
    "            inducing_points,\n",
    "            torch.zeros_like(inducing_points),\n",
    "            'r*',\n",
    "            label='inducing_points'\n",
    "        )\n",
    "def gp_regression(train_x, train_y, test_x, lengthscale, outputscale, noise, kernel, composition):\n",
    "    kernel = get_kernel(kernel, composition)\n",
    "    model = ExactGP(train_x, train_y, kernel)\n",
    "\n",
    "    # Set hyper-parameters\n",
    "    model.length_scale = lengthscale\n",
    "    model.output_scale = outputscale\n",
    "    model.likelihood.noise = torch.tensor([noise])\n",
    "    \n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T07:24:43.527020Z",
     "start_time": "2021-10-15T07:24:43.107247Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e20779cf384e96bd1e1a0338c751dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, continuous_update=False, description='lengthscale', max=2.0, min=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rcParams['figure.figsize'] = (15, 8)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "num_training = 25\n",
    "train_x = (torch.rand(num_training) - 0.5) * 10\n",
    "train_y = regression_function(train_x)\n",
    "test_x = torch.linspace(-6, 6, 1000)\n",
    "\n",
    "def gp_regression_(lengthscale, outputscale, noise, kernel, composition):\n",
    "    model = gp_regression(train_x, train_y, test_x, lengthscale, outputscale, noise, kernel, composition)\n",
    "    # Evaluate GP Model.\n",
    "    plot_model(model, train_x, train_y, test_x)\n",
    "    test_y = regression_function(test_x, noise=0).detach()\n",
    "    plt.plot(test_x, test_y,'k-', label='Noise-free Function')\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "kernels = [\"RBF\", \"linear\", \"quadratic\", \"Matern-1/2\", \"Matern-3/2\", \"Matern-5/2\", \"Cosine\"]\n",
    "composition = [\"addition\", \"product\"]\n",
    "\n",
    "interact(\n",
    "    gp_regression_,\n",
    "    num_training=ipywidgets.IntSlider(\n",
    "        value=25, min=1, max=1000, continuous_update=False),\n",
    "    lengthscale=ipywidgets.FloatSlider(\n",
    "        value=1., min=0.01, max=2, step=0.01, continuous_update=False),\n",
    "    outputscale=ipywidgets.FloatSlider(\n",
    "        value=1., min=0.01, max=5, step=0.01, continuous_update=False),\n",
    "    noise=ipywidgets.FloatLogSlider(\n",
    "        value=0.1, min=-3, max=2, continuous_update=False),\n",
    "    kernel=ipywidgets.SelectMultiple(\n",
    "        options=kernels,\n",
    "        value=[\"RBF\"],\n",
    "        rows=len(kernels),\n",
    "        disabled=False),\n",
    "    composition=ipywidgets.Dropdown(\n",
    "        options=composition,\n",
    "        value=composition[0],\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Optimization of Hyper Parameters with type-II MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "x = pd.read_csv(\"train_x.csv\")[:10000][:].values\n",
    "y = pd.read_csv(\"train_y.csv\")[:10000].values\n",
    "print(len(x))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T07:24:45.960927Z",
     "start_time": "2021-10-15T07:24:44.092049Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-0f55b6256010>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Calc loss and backprop gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mmll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_validate_module_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\mlls\\exact_marginal_log_likelihood.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, function_dist, target, *params)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# Get the log prob of the marginal distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_other_terms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\distributions\\multivariate_normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;31m# Get log determininant and first part of quadratic form\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mcovar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcovar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_kernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0minv_quad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogdet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcovar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv_quad_logdet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minv_quad_rhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minv_quad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\lazy\\batch_repeat_lazy_tensor.py\u001b[0m in \u001b[0;36minv_quad_logdet\u001b[1;34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[0;32m    256\u001b[0m             \u001b[0minv_quad_rhs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_move_repeat_batches_to_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minv_quad_rhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m         \u001b[0minv_quad_term\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogdet_term\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_lazy_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv_quad_logdet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minv_quad_rhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_inv_quad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minv_quad_term\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minv_quad_term\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\lazy\\lazy_tensor.py\u001b[0m in \u001b[0;36minv_quad_logdet\u001b[1;34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[0;32m   1280\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInvQuadLogDet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1282\u001b[1;33m         inv_quad_term, logdet_term = func(\n\u001b[0m\u001b[0;32m   1283\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepresentation_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\functions\\_inv_quad_log_det.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(ctx, representation_tree, dtype, device, matrix_shape, batch_shape, inv_quad, logdet, probe_vectors, probe_vector_norms, *args)\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mt_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogdet\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msettings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mskip_logdet_forward\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m             \u001b[0msolves\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlazy_tsr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_solve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreconditioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_tridiag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_random_probes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\lazy\\lazy_tensor.py\u001b[0m in \u001b[0;36m_solve\u001b[1;34m(self, rhs, preconditioner, num_tridiag)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_solve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreconditioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_tridiag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m         return utils.linear_cg(\n\u001b[0m\u001b[0;32m    659\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matmul\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m             \u001b[0mrhs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\utils\\linear_cg.py\u001b[0m in \u001b[0;36mlinear_cg\u001b[1;34m(matmul_closure, rhs, n_tridiag, tolerance, eps, stop_updating_after, max_iter, max_tridiag_iter, initial_guess, preconditioner)\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;31m# Get next alpha\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[1;31m# alpha_{k} = (residual_{k-1}^T precon_residual{k-1}) / (p_vec_{k-1}^T mat p_vec_{k-1})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[0mmvms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatmul_closure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurr_conjugate_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprecond\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurr_conjugate_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmvms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmul_storage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\lazy\\added_diag_lazy_tensor.py\u001b[0m in \u001b[0;36m_matmul\u001b[1;34m(self, rhs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_diag_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_diag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_diag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madded_diag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\lazy\\constant_mul_lazy_tensor.py\u001b[0m in \u001b[0;36m_matmul\u001b[1;34m(self, rhs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_lazy_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpanded_constant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\lazy\\non_lazy_tensor.py\u001b[0m in \u001b[0;36m_matmul\u001b[1;34m(self, rhs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prod_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "num_training = 50\n",
    "train_x = torch.Tensor(x)\n",
    "train_y = torch.Tensor(y)\n",
    "test_x = torch.linspace(-6, 6, 100)\n",
    "\n",
    "linestyles = ['-', ':', '--', '-.', \n",
    "             (0, (1, 10)), (0, (5, 10)), (0, (3, 5, 1, 5)), (0, (3, 5, 1, 5, 1, 5))\n",
    "             ]\n",
    "\n",
    "best_kernel = {}\n",
    "for i, kernel in enumerate(kernels):\n",
    "    model = ExactGP(train_x, train_y, get_kernel(kernel))\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam([{'params': model.parameters()}], lr=0.1)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    training_iter = 100\n",
    "\n",
    "    losses = []\n",
    "    lengthscale = []\n",
    "    outputscale = []\n",
    "    noise = []\n",
    "\n",
    "    for _ in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        lengthscale.append(model.length_scale.item())\n",
    "        outputscale.append(model.output_scale.item())\n",
    "        noise.append(model.likelihood.noise.item())\n",
    "        optimizer.step()\n",
    "    plt.plot(losses, label=f\"{kernel}\", linestyle=linestyles[i], linewidth=4)\n",
    "    best_kernel[kernel] = (loss.item(), model)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Num Iteration\")\n",
    "plt.ylabel(\"MLL Loss\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "best_model = min(best_kernel.values(), key=lambda x: x[0])[1]\n",
    "plot_model(best_model, train_x, train_y, test_x)\n",
    "test_y = regression_function(test_x, noise=0).detach()\n",
    "plt.plot(test_x, test_y,'k-', label='Noise-free Function')\n",
    "plt.legend(loc='upper left')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Weather with GPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T07:24:46.666732Z",
     "start_time": "2021-10-15T07:24:45.985741Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/GlobalTemperatures.csv\")\n",
    "df = df.dropna()\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "year = []\n",
    "month = []\n",
    "dt = pd.to_datetime(df.dt)\n",
    "for i in range(len(dt)):\n",
    "    month.append(dt[i].month)\n",
    "    year.append(dt[i].year)\n",
    "df[\"year\"] = year \n",
    "df[\"month\"] = month\n",
    "\n",
    "\n",
    "def gp_weather(period, kernel, composition):\n",
    "    \n",
    "    if period == \"yearly\":\n",
    "        idx = df[\"month\"] == 7\n",
    "        x0 = df[\"year\"][idx]\n",
    "        xlabel = 'Year'\n",
    "    else:\n",
    "        idx = df[\"year\"] > 2010\n",
    "        x0 = df[\"month\"][idx] + 12 * (df[\"year\"][idx] - 2010)\n",
    "        xlabel = 'Month'\n",
    "\n",
    "    y = df[\"LandAverageTemperature\"][idx].values\n",
    "\n",
    "    x = np.arange(len(y))\n",
    "\n",
    "    x_min, x_max = x.min(), x.max()\n",
    "    y_mean, y_std = y.mean(), y.std()\n",
    "\n",
    "    train_x = 6 * (torch.tensor((x - x_min) / (x_max - x_min)) - 0.5).to(torch.float)\n",
    "    reverse_x = lambda x: (x / 6 + 0.5) * (x_max - x_min) + x_min\n",
    "    train_y = torch.tensor( (y - y_mean) / y_std).to(torch.float)\n",
    "    reverse_y = lambda y: y * y_std + y_mean\n",
    "    test_x = torch.linspace(-3, 5, 1000)\n",
    "\n",
    "    model = ExactGP(train_x, train_y, get_kernel(kernel))\n",
    "    \n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam([{'params': model.parameters()}], lr=0.1)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    training_iter = 100\n",
    "    for _ in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(test_x)\n",
    "        lower, upper = out.confidence_region()\n",
    "        y_dist = model.likelihood(out)\n",
    "        y_lower, y_upper = y_dist.confidence_region()\n",
    "    plt.plot(reverse_x(train_x) , reverse_y(train_y), 'k*', label='Train Data')\n",
    "    plt.plot(reverse_x(test_x), reverse_y(out.mean), 'b-', label='Mean Prediction')\n",
    "    plt.plot(reverse_x(test_x), reverse_y(out.sample()), 'r--', label='GP Sample')\n",
    "    for _ in range(3):\n",
    "        plt.plot(reverse_x(test_x), reverse_y(out.sample()), 'r--')\n",
    "    plt.fill_between(reverse_x(test_x).numpy(), reverse_y(lower.numpy()), reverse_y(upper.numpy()), \n",
    "                     color='b', alpha=0.2, label='Epistemic Uncertainty')\n",
    "    \n",
    "    \n",
    "    plt.fill_between(reverse_x(test_x).numpy(), reverse_y(y_lower.numpy()), reverse_y(lower.numpy()), \n",
    "                     color='g', alpha=0.2, label='Aleatoric Uncertainty')\n",
    "    plt.fill_between(reverse_x(test_x).numpy(), reverse_y(upper.numpy()), reverse_y(y_upper.numpy()), \n",
    "                     color='g', alpha=0.2)\n",
    "    \n",
    "    plt.legend(loc='upper left')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Global Temperature\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "interact(\n",
    "    gp_weather,\n",
    "    period=ipywidgets.Dropdown(\n",
    "        options=['yearly', 'monthly after 2010']\n",
    "    ),\n",
    "    kernel=ipywidgets.SelectMultiple(\n",
    "        options=kernels,\n",
    "        value=[\"RBF\"],\n",
    "        rows=len(kernels),\n",
    "        disabled=False),\n",
    "    composition=ipywidgets.Dropdown(\n",
    "        options=composition,\n",
    "        value=composition[0],\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse GPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T07:24:46.934363Z",
     "start_time": "2021-10-15T07:24:46.710141Z"
    }
   },
   "outputs": [],
   "source": [
    "from gpytorch import settings\n",
    "from gpytorch.lazy import MatmulLazyTensor, delazify, lazify\n",
    "from gpytorch.models.exact_prediction_strategies import (\n",
    "    DefaultPredictionStrategy,\n",
    "    clear_cache_hook,\n",
    ")\n",
    "from gpytorch.utils.memoize import cached\n",
    "from scipy.stats.distributions import chi\n",
    "\n",
    "\n",
    "class SparsePredictionStrategy(DefaultPredictionStrategy):\n",
    "    \"\"\"Prediction strategy for Sparse GPs.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_inputs,\n",
    "        train_prior_dist,\n",
    "        train_labels,\n",
    "        likelihood,\n",
    "        k_uu,\n",
    "        root=None,\n",
    "        inv_root=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            train_inputs,\n",
    "            train_prior_dist,\n",
    "            train_labels,\n",
    "            likelihood,\n",
    "            root=root,\n",
    "            inv_root=inv_root,\n",
    "        )\n",
    "        self.k_uu = k_uu\n",
    "        self.lik_train_train_covar = train_prior_dist.lazy_covariance_matrix\n",
    "        \n",
    "    @property  # type: ignore\n",
    "    @cached(name=\"k_uu_inv_root\")\n",
    "    def k_uu_inv_root(self):\n",
    "        \"\"\"Get K_uu^-1/2.\"\"\"\n",
    "        train_train_covar = self.k_uu\n",
    "        train_train_covar_inv_root = delazify(\n",
    "            train_train_covar.root_inv_decomposition().root\n",
    "        )\n",
    "        return train_train_covar_inv_root\n",
    "\n",
    "    @property  # type: ignore\n",
    "    @cached(name=\"mean_cache\")\n",
    "    def mean_cache(self):\n",
    "        r\"\"\"Get mean cache, namely \\sigma^-1 k_uf y_f.\"\"\"\n",
    "        sigma = self.lik_train_train_covar\n",
    "        sigma_inv_root = delazify(sigma.root_inv_decomposition().root)\n",
    "        sigma_inv = sigma_inv_root @ sigma_inv_root.transpose(-2, -1)\n",
    "        mean_cache = (sigma_inv @ self.train_labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        if settings.detach_test_caches.on():\n",
    "            mean_cache = mean_cache.detach()\n",
    "\n",
    "        if mean_cache.grad_fn is not None:\n",
    "            wrapper = functools.partial(clear_cache_hook, self)\n",
    "            functools.update_wrapper(wrapper, clear_cache_hook)\n",
    "            mean_cache.grad_fn.register_hook(wrapper)\n",
    "\n",
    "        return mean_cache\n",
    "\n",
    "\n",
    "class SparseGP(ExactGP):\n",
    "    def __init__(self, train_x, train_y, kernel, inducing_points, approximation=\"DTC\", jitter=1e-3):\n",
    "        super().__init__(train_x, train_y, kernel)\n",
    "        self.prediction_strategy = None\n",
    "        self.xu = inducing_points\n",
    "        self.approximation = approximation\n",
    "        self.jitter = jitter\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"Return GP posterior at location `x'.\"\"\"\n",
    "        train_inputs = self.xu\n",
    "        m = train_inputs.shape[0]\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(-1)\n",
    "        inputs = x\n",
    "\n",
    "        if self.prediction_strategy is None:\n",
    "            x_uf = torch.cat((train_inputs, self.train_inputs[0]), dim=0)\n",
    "            output = self.forward(x_uf)\n",
    "            mu_uf, kernel = output.mean, output.lazy_covariance_matrix\n",
    "\n",
    "            mu_u, mu_f = mu_uf[:m], mu_uf[m:]\n",
    "            k_uu, k_ff, k_uf = kernel[:m, :m], kernel[m:, m:], kernel[:m, m:]\n",
    "\n",
    "            if self.approximation == \"FITC\":\n",
    "                k_uu_root_inv = k_uu.root_inv_decomposition().root\n",
    "                z = k_uf.transpose(-2, -1) @ k_uu_root_inv\n",
    "                q_ff = z @ z.transpose(-2, -1)\n",
    "\n",
    "                diag = delazify(k_ff - q_ff).diag() + self.likelihood.noise\n",
    "                diag = lazify(torch.diag(1 / diag))\n",
    "\n",
    "            elif self.approximation == \"SOR\" or self.approximation == \"DTC\":\n",
    "                diag = lazify(torch.eye(len(self.train_targets))).mul(\n",
    "                    1.0 / self.likelihood.noise\n",
    "                )\n",
    "            else:\n",
    "                raise NotImplementedError(\n",
    "                    f\"{self.approximation} Not implemented.\")\n",
    "\n",
    "            cov = k_uu + (k_uf @ diag) @ k_uf.transpose(-2, -1)\n",
    "\n",
    "            prior_dist = gpytorch.distributions.MultivariateNormal(\n",
    "                mu_u, cov.add_jitter(self.jitter)\n",
    "            )\n",
    "\n",
    "            # Create the prediction strategy for\n",
    "            self.prediction_strategy = SparsePredictionStrategy(\n",
    "                train_inputs=train_inputs,\n",
    "                train_prior_dist=prior_dist,\n",
    "                train_labels=(k_uf @ diag) @ (self.train_targets - mu_f),\n",
    "                likelihood=self.likelihood,\n",
    "                k_uu=k_uu.add_jitter(self.jitter),\n",
    "            )\n",
    "\n",
    "        # Concatenate the input to the training input\n",
    "        batch_shape = inputs.shape[:-2]\n",
    "        # Make sure the batch shapes agree for training/test data\n",
    "        if batch_shape != train_inputs.shape[:-2]:\n",
    "            train_inputs = train_inputs.expand(\n",
    "                *batch_shape, *train_inputs.shape[-2:])\n",
    "        full_inputs = torch.cat([train_inputs, inputs], dim=-2)\n",
    "\n",
    "        # Get the joint distribution for training/test data\n",
    "        joint_output = self.forward(full_inputs)\n",
    "        joint_mean, joint_covar = joint_output.loc, joint_output.lazy_covariance_matrix\n",
    "\n",
    "        # Separate components.\n",
    "        mu_s = joint_mean[..., m:]\n",
    "        k_su, k_ss = joint_covar[..., m:, :m], joint_covar[..., m:, m:]\n",
    "\n",
    "        pred_mean = mu_s + k_su @ self.prediction_strategy.mean_cache\n",
    "\n",
    "        sig_inv_root = self.prediction_strategy.covar_cache\n",
    "        k_su_sig_inv_root = k_su @ sig_inv_root\n",
    "        rhs = MatmulLazyTensor(\n",
    "            k_su_sig_inv_root, k_su_sig_inv_root.transpose(-2, -1))\n",
    "\n",
    "        kuu_inv_root = self.prediction_strategy.k_uu_inv_root\n",
    "        k_su_kuu_inv_root = k_su @ kuu_inv_root\n",
    "        q_ss = MatmulLazyTensor(\n",
    "            k_su_kuu_inv_root, k_su_kuu_inv_root.transpose(-2, -1))\n",
    "\n",
    "        if self.approximation == \"DTC\" or self.approximation == \"FITC\":\n",
    "            pred_cov = k_ss - q_ss + rhs\n",
    "        elif self.approximation == \"SOR\":\n",
    "            pred_cov = rhs\n",
    "        else:\n",
    "            raise NotImplementedError(f\"{self.approximation} Not implemented.\")\n",
    "\n",
    "        return joint_output.__class__(pred_mean, pred_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T07:24:47.349440Z",
     "start_time": "2021-10-15T07:24:46.982237Z"
    }
   },
   "outputs": [],
   "source": [
    "import time \n",
    "rcParams['figure.figsize'] = (15, 8)\n",
    "\n",
    "\n",
    "def sparse_gp_regression(num_training, num_inducing_points, approximation, method, plot_train_data):\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    train_x = (torch.rand(num_training) - 0.5) * 10\n",
    "    train_y = regression_function(train_x)\n",
    "    test_x = torch.linspace(-6, 6, 100)\n",
    "\n",
    "    # Subsample inducing points\n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "    if approximation == \"ExactGP\":\n",
    "        model = ExactGP(train_x, train_y, get_kernel(\"RBF\"))\n",
    "        inducing_points = None \n",
    "    else:\n",
    "        if method == \"uniform\":\n",
    "            inducing_points = torch.linspace(-6, 6, num_inducing_points).unsqueeze(-1)\n",
    "        elif method == \"random\":\n",
    "            inducing_points = ((torch.rand(num_inducing_points) - 0.5) * 10).unsqueeze(-1)\n",
    "        model = SparseGP(train_x, train_y, get_kernel(\"RBF\"), inducing_points, approximation,\n",
    "        jitter=0.1)\n",
    "\n",
    "    model.length_scale = 1.2\n",
    "    model.output_scale = 0.9 \n",
    "    model.likelihood.noise = torch.tensor([0.05])\n",
    "        \n",
    "    plot_model(model, train_x, train_y, test_x, inducing_points, plot_points=plot_train_data)\n",
    "    plt.title(f\"{approximation} Inference Time: {time.time() - start} s\")\n",
    "    plt.plot(test_x, test_y,'k-', label='Noise-free Function')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "interact(\n",
    "    sparse_gp_regression,\n",
    "    num_training=ipywidgets.IntSlider(\n",
    "        value=500, \n",
    "        min=100, step=100, max=10000, continuous_update=False, \n",
    "        description='Number training points:', style={'description_width': 'initial'}\n",
    "    ),\n",
    "    num_inducing_points=ipywidgets.IntSlider(\n",
    "        value=5,\n",
    "        min=1, max=20, continuous_update=False, \n",
    "        description='Number inducing points:', style={'description_width': 'initial'}),\n",
    "    approximation=ipywidgets.Dropdown(\n",
    "        options=[\"ExactGP\", \"DTC\", \"FITC\", \"SOR\"], \n",
    "        value=\"DTC\",\n",
    "        description='Sparse Approximation:', style={'description_width': 'initial'}),\n",
    "    method=[\"uniform\", \"random\"],\n",
    "    plot_train_data=ipywidgets.Checkbox(\n",
    "        value=True, description='plot train data',\n",
    "    ),\n",
    "    \n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Feature Approximation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T07:26:04.046585Z",
     "start_time": "2021-10-15T07:26:04.028121Z"
    }
   },
   "outputs": [],
   "source": [
    "class RandomFeatureGP(ExactGP):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_x,\n",
    "        train_y,\n",
    "        kernel,\n",
    "        num_features,\n",
    "        approximation=\"RFF\",\n",
    "    ):\n",
    "        super().__init__(train_x, train_y, kernel)\n",
    "        self.num_features = num_features\n",
    "        self.approximation = approximation\n",
    "\n",
    "        self.dim = train_x.shape[-1]\n",
    "        self.w, self.b, self._feature_scale = self._sample_features()\n",
    "        self.full_predictive_covariance = True\n",
    "\n",
    "    @property\n",
    "    def scale(self):\n",
    "        \"\"\"Return feature scale.\"\"\"\n",
    "        return torch.sqrt(self._feature_scale * self.output_scale)\n",
    "\n",
    "    def sample_features(self):\n",
    "        \"\"\"Sample a new set of features.\"\"\"\n",
    "        self.w, self.b, self._feature_scale = self._sample_features()\n",
    "\n",
    "    def _sample_features(self):\n",
    "        \"\"\"Sample a new set of random features.\"\"\"\n",
    "        # Only squared-exponential kernels are implemented.\n",
    "        if self.approximation == \"RFF\":\n",
    "            w = torch.randn(self.num_features, self.dim) / \\\n",
    "                torch.sqrt(self.length_scale)\n",
    "            scale = torch.tensor(1.0 / self.num_features)\n",
    "\n",
    "        elif self.approximation == \"OFF\":\n",
    "            q, _ = torch.qr(torch.randn(self.num_features, self.dim))\n",
    "            diag = torch.diag(\n",
    "                torch.tensor(\n",
    "                    chi.rvs(df=self.num_features, size=self.num_features),\n",
    "                    dtype=torch.get_default_dtype(),\n",
    "                )\n",
    "            )\n",
    "            w = (diag @ q) / torch.sqrt(self.length_scale)\n",
    "            scale = torch.tensor(1.0 / self.num_features)\n",
    "\n",
    "        elif self.approximation == \"QFF\":\n",
    "            q = int(np.floor(np.power(self.num_features, 1.0 / self.dim)))\n",
    "            self._num_features = q ** self.dim\n",
    "            omegas, weights = np.polynomial.hermite.hermgauss(2 * q)\n",
    "            omegas = torch.tensor(omegas[:q], dtype=torch.get_default_dtype())\n",
    "            weights = torch.tensor(\n",
    "                weights[:q], dtype=torch.get_default_dtype())\n",
    "\n",
    "            omegas = torch.sqrt(1.0 / self.length_scale) * omegas\n",
    "            w = torch.cartesian_prod(*[omegas.squeeze()\n",
    "                                       for _ in range(self.dim)])\n",
    "            if self.dim == 1:\n",
    "                w = w.unsqueeze(-1)\n",
    "\n",
    "            weights = 4 * weights / np.sqrt(np.pi)\n",
    "            scale = torch.cartesian_prod(*[weights for _ in range(self.dim)])\n",
    "            if self.dim > 1:\n",
    "                scale = scale.prod(dim=1)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"{self.approximation} not implemented.\")\n",
    "\n",
    "        b = 2 * torch.tensor(np.pi) * torch.rand(self.num_features)\n",
    "        self.prediction_strategy = None  # reset prediction strategy.\n",
    "        return w, b, scale\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"Return GP posterior at location `x'.\"\"\"\n",
    "        train_inputs = torch.zeros(2 * self.num_features, 1)\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(-1)\n",
    "        inputs = x\n",
    "\n",
    "        if self.prediction_strategy is None:\n",
    "            x = self.train_inputs[0]\n",
    "            zt = self.forward(x).transpose(-2, -1)\n",
    "\n",
    "            mean = train_inputs.squeeze(-1)\n",
    "\n",
    "            cov = lazify(zt @ zt.transpose(-1, -2)).add_jitter()\n",
    "\n",
    "            y = self.train_targets - self.mean_module(x)\n",
    "            labels = zt @ y\n",
    "\n",
    "            prior_dist = gpytorch.distributions.MultivariateNormal(mean, cov)\n",
    "            self.prediction_strategy = DefaultPredictionStrategy(\n",
    "                train_inputs=train_inputs,\n",
    "                train_prior_dist=prior_dist,\n",
    "                train_labels=labels,\n",
    "                likelihood=self.likelihood,\n",
    "            )\n",
    "        #\n",
    "        z = self.forward(inputs)\n",
    "        pred_mean = self.mean_module(\n",
    "            inputs) + z @ self.prediction_strategy.mean_cache\n",
    "\n",
    "        if self.full_predictive_covariance:\n",
    "            precomputed_cache = self.prediction_strategy.covar_cache\n",
    "            covar_inv_quad_form_root = z @ precomputed_cache\n",
    "\n",
    "            pred_cov = (\n",
    "                MatmulLazyTensor(\n",
    "                    covar_inv_quad_form_root, covar_inv_quad_form_root.transpose(\n",
    "                        -1, -2)\n",
    "                )\n",
    "                .mul(self.likelihood.noise)\n",
    "                .add_jitter(1e-12)\n",
    "            )\n",
    "        else:\n",
    "            dim = pred_mean.shape[-1]\n",
    "            pred_cov = 1e-6 * torch.eye(dim)\n",
    "\n",
    "        return gpytorch.distributions.MultivariateNormal(pred_mean, pred_cov)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Compute features at location x.\"\"\"\n",
    "        z = x @ self.w.transpose(-2, -1) + self.b\n",
    "        return torch.cat([self.scale * torch.cos(z), self.scale * torch.sin(z)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T07:26:06.605145Z",
     "start_time": "2021-10-15T07:26:06.296078Z"
    }
   },
   "outputs": [],
   "source": [
    "def rff_gp_regression(num_training, num_features, approximation, plot_train_data):\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    train_x = (torch.rand(num_training) - 0.5) * 10\n",
    "    train_y = regression_function(train_x)\n",
    "    test_x = torch.linspace(-6, 6, 100)\n",
    "\n",
    "    def sample_features():\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        plt.close()\n",
    "        \n",
    "        start = time.time()\n",
    "        if approximation == \"ExactGP\":\n",
    "            model = ExactGP(train_x, train_y, get_kernel(\"RBF\"))\n",
    "        else:\n",
    "            model = RandomFeatureGP(train_x.unsqueeze(-1), \n",
    "                            train_y, get_kernel(\"RBF\"), num_features, approximation)\n",
    "        model.length_scale = 1.2\n",
    "        model.output_scale = 0.9 \n",
    "        model.likelihood.noise = torch.tensor([0.01])\n",
    "        try:\n",
    "            model.sample_features()\n",
    "        except AttributeError:\n",
    "            pass \n",
    "        \n",
    "        plot_model(model, train_x, train_y, test_x, plot_points=plot_train_data)\n",
    "        plt.plot(test_x, test_y,'k-', label='Noise-free Function')\n",
    "        plt.title(f\"{approximation} Inference Time: {time.time() - start} s\")\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.show()\n",
    "    \n",
    "        button = ipywidgets.Button(description=\"Sample Features\")\n",
    "        button.on_click(lambda b: sample_features())\n",
    "        display(button)\n",
    "\n",
    "    sample_features()\n",
    "\n",
    "\n",
    "interact(\n",
    "    rff_gp_regression,\n",
    "    num_training=ipywidgets.IntSlider(\n",
    "        value=500, min=100, step=100, max=10000, continuous_update=False, \n",
    "        description='Number training points:', style={'description_width': 'initial'}\n",
    "    ),\n",
    "    num_features=ipywidgets.IntSlider(\n",
    "        value=20, min=2, max=num_training, continuous_update=False, \n",
    "        description='Number features:', style={'description_width': 'initial'}),\n",
    "    approximation=ipywidgets.Dropdown(\n",
    "        options=[\"ExactGP\", \"RFF\",  \"QFF\", \"OFF\"], \n",
    "        value=\"RFF\",\n",
    "        description='Kernel Approximation:', style={'description_width': 'initial'}),\n",
    "    plot_train_data=ipywidgets.Checkbox(\n",
    "        value=True, description='plot train data',\n",
    "    ),\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8697ffd23d66446d592afda574e6b966b5d3c7c7a76ece1c6d9691e118759c03"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
